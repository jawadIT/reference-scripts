pd.read_csv(f'{path}\\{filename}',low_memory=False,chunksize=20000)

df.columns = [c.lower() for c in df.columns]

all_null_cols = df.columns[df.isnull().all()]

df = df.drop(all_null_cols,axis=1)

df = df.replace([np.inf, -np.inf],np.nan)

df = df.dropna(subset=['cols'],how=all)

df.sort_values(by=[cols],inplace=True,ascending=True)

df[col].replace(np.nan, 0 , inplace = True)


df.rename(columns = {"oldname":"newname"}, inplace=True)

df['datetimecolumn'] = pd.to_datetime(df['datetimecolumn'],format='%Y-%m-%d %H.%M.%S')

df['timedifferencecolumn'].dt.total_seconds()

pd.concat([df1,df2],axis=0)

df.index = df['column name']

df['col_name'] = df.apply(lambda x: x['col'] , axis = 1)

df.loc[df['col'] == value  , 'new col name'] = df[ df['col'] == value].apply(lambda x: x[''] , axis = 1)

df[ df['datetimecolumns'][0] - timedelta(hours=) : ]

df['col'].astype(int)

df.to_csv(f'{path}\\{filename}')

df.groupby(['year','month']).agg({'col1':['sum'],'col2':['sum']})

statistics.mean(list)

statistics.stdev(list)

lst = []

np.percentile(lst,80)

lst[lst1.index('value')]

Counter(lst)

percentileofscore(lst,value)

df['col'].rolling('0h0min2s').sum()

df['col'].rolling(10).sum()


import pandas as pd
import numpy as np
import math
import statistics
import scipy.stats import percentileofscore
from collections import Counter
from dateutil import tz


from keras import optimizers


from pickle import dump,load

import datetime

from datetime import timedelta


import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer

scaler = StandardScaler()
scaler = MinMaxScaler()
scaler = Normalizer()

scaler.fit(x)

scaler.transform(x)



path = 'C:\\Users\\jawad_zf1uaw5\\Desktop\\data'
filename = 'data2.json'
filetype = 'json'
df = get_file(path,filename,filetype)
print(df.head())

import json 
with open(filename, 'r') as f:
    for line in f:
        data = json.loads(line)



path = 'C:\\Users\\jawad_zf1uaw5\\Desktop\\data'
filename = 'datain.xlsb'
filetype = 'xlsb'
df = get_file(path,filename,filetype)
print(df.head())


fig, ax = plt.subplots(figsize=(14, 6), dpi=80)
ax.plot(df['col1'], label='col1', color='blue', animated = True, linewidth=1)
ax.plot(df['col2'], label='col2', color='red', animated = True, linewidth=1)
plt.legend(loc='lower left')
ax.set_title('title', fontsize=16)
plt.show()



pd.to_numeric(df["a"])
df.apply(pd.to_numeric)
df[["a", "b"]].apply(pd.to_numeric)
pd.to_numeric(s, errors='coerce')
df.astype(int)


df = df._get_numeric_data()

columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [0])], remainder='passthrough')

df = np.array(columnTransformer.fit_transform(df), dtype = np.str)

le = LabelEncoder()
le.fit(df[''])
df[''] = le.transform(df[''])


features = "+".join(df.columns - ["target variable"])
y, X = dmatrices('target variable ~' + features, df, return_type='dataframe')
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif["features"] = X.columns


sign = lambda x: (1, -1)[x < 0]
labelcol = 'y'
vector = df[labelcol].copy()
for s in range(abs(-10)):
    tmp = vector.shift(sign(shift_by))
    tmp = tmp.fillna(0)
    vector += tmp
# Add vector to the df
df.insert(loc=0, column=labelcol+'tmp', value=vector)
# Remove the rows with labelcol == 1.
df = df.drop(df[df[labelcol] == 1].index)
# Drop labelcol and rename the tmp col as labelcol
df = df.drop(labelcol, axis=1)
df = df.rename(columns={labelcol+'tmp': labelcol})
# Make the labelcol binary
df.loc[df[labelcol] > 0, labelcol] = 1



input_X = df.loc[:, df.columns != labelcol].values
input_y = df[labelcol].values
n_features = input_X.shape[1]


lookback = 5
output_X = []
output_y = []
for i in range(len(input_X) - lookback - 1):
    t = []
    for j in range(1, lookback + 1):
        t.append(input_X[[(i + j + 1)], :])
    output_X.append(t)
    output_y.append(input_y[i + lookback + 1])
x , y = np.squeeze(np.array(output_X)), np.array(output_y)

X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=DATA_SPLIT_PCT, random_state=SEED)

X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=DATA_SPLIT_PCT, random_state=SEED)


X_train = X_train.reshape(X_train.shape[0], lookback, n_features)

flattened_X = np.empty((X.shape[0], X.shape[2]))

for i in range(X.shape[0]):
        flattened_X[i] = X[i, (X.shape[1]-1), :]
return(flattened_X)

for i in range(X.shape[0]):
        X[i, :, :] = scaler.transform(X[i, :, :])
    return X

scaler = StandardScaler()
scaler.fit(X)
xscaled = scaler.transform(X)

a = flatten(X_train_y0_scaled)
print('colwise mean', np.mean(a, axis=0).round(6))
print('colwise variance', np.var(a, axis=0))


timesteps =  X_train_y0_scaled.shape[1] # equal to the lookback
n_features =  X_train_y0_scaled.shape[2] # 59

epochs = 200
batch = 64
lr = 0.0001


lstm_autoencoder = Sequential()
# Encoder
lstm_autoencoder.add(LSTM(32, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))
lstm_autoencoder.add(LSTM(16, activation='relu', return_sequences=False))
lstm_autoencoder.add(RepeatVector(timesteps))
# Decoder
lstm_autoencoder.add(LSTM(16, activation='relu', return_sequences=True))
lstm_autoencoder.add(LSTM(32, activation='relu', return_sequences=True))
lstm_autoencoder.add(TimeDistributed(Dense(n_features)))

lstm_autoencoder.summary()


adam = optimizers.Adam(lr)
lstm_autoencoder.compile(loss='mse', optimizer=adam)

cp = ModelCheckpoint(filepath="lstm_autoencoder_classifier.h5",
                               save_best_only=True,
                               verbose=0)

tb = TensorBoard(log_dir='./logs',
                histogram_freq=0,
                write_graph=True,
                write_images=True)

lstm_autoencoder_history = lstm_autoencoder.fit(X_train_y0_scaled, X_train_y0_scaled, 
                                                epochs=epochs, 
                                                batch_size=batch, 
                                                validation_data=(X_valid_y0_scaled, X_valid_y0_scaled),
                                                verbose=2).history

valid_x_predictions = lstm_autoencoder.predict(X_valid_scaled)
mse = np.mean(np.power(flatten(X_valid_scaled) - flatten(valid_x_predictions), 2), axis=1)

error_df = pd.DataFrame({'Reconstruction_error': mse,
                        'True_class': y_valid.tolist()})

precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class, error_df.Reconstruction_error)
plt.plot(threshold_rt, precision_rt[1:], label="Precision",linewidth=5)
plt.plot(threshold_rt, recall_rt[1:], label="Recall",linewidth=5)
plt.title('Precision and recall for different threshold values')
plt.xlabel('Threshold')
plt.ylabel('Precision/Recall')
plt.legend()
plt.show()


test_x_predictions = lstm_autoencoder.predict(X_test_scaled)
mse = np.mean(np.power(flatten(X_test_scaled) - flatten(test_x_predictions), 2), axis=1)

error_df = pd.DataFrame({'Reconstruction_error': mse,
                        'True_class': y_test.tolist()})

threshold_fixed = 0.3
groups = error_df.groupby('True_class')
fig, ax = plt.subplots()

for name, group in groups:
    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',
            label= "Break" if name == 1 else "Normal")
ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors="r", zorder=100, label='Threshold')
ax.legend()
plt.title("Reconstruction error for different classes")
plt.ylabel("Reconstruction error")
plt.xlabel("Data point index")
plt.show()

pred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]
conf_matrix = confusion_matrix(error_df.True_class, pred_y)

plt.figure(figsize=(6, 6))
sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()


false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.True_class, error_df.Reconstruction_error)
roc_auc = auc(false_pos_rate, true_pos_rate,)

plt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)
plt.plot([0,1],[0,1], linewidth=5)

plt.xlim([-0.01, 1])
plt.ylim([0, 1.01])
plt.legend(loc='lower right')
plt.title('Receiver operating characteristic curve (ROC)')
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()


import xml.etree.ElementTree as et 
xtree = et.parse("test.xml")
xroot = xtree.getroot()

df_cols = ["name", "email", "grade", "age"]
rows = []

for node in xroot: 
    s_name = node.attrib.get("name")
    s_mail = node.find("email").text
    s_grade = node.find("grade").text
    s_age = node.find("age").text
    rows.append({"name": s_name, "email": s_mail, 
                 "grade": s_grade, "age": s_age})

out_df = pd.DataFrame(rows, columns = df_cols)

    for node in xroot: 
        res = []
        res.append(node.attrib.get(df_cols[0]))
        for el in df_cols[1:]: 
            if node is not None and node.find(el) is not None:
                res.append(node.find(el).text)
            else: 
                res.append(None)
        rows.append({df_cols[i]: res[i] 
                     for i, _ in enumerate(df_cols)})


for x in root.iter('region'):
    root1=et.Element('root')
    root1=x
    for supply in root1.iter('AgSupplySector'):
        root2=et.Element('root')
        print(supply)
        root2=(supply)
        for tech in root2.iter('AgProductionTechnology'):
            root3 = et.Element('root')
            root3=(tech)
            for yr in root3.iter('period'):
                root4 = et.Element('root')
                root4=yr
                for gas in root4.iter('Non-CO2'):
                    root5 = et.Element('root')
                    root5=gas
                    for em in root5.iter('input-emissions'):
                        Technology.append(tech.attrib['name'])
                        Value.append(em.text)
                        Gas.append(gas.attrib['name'])
                        Year.append(yr.attrib['year'])
                        Crop.append(supply.attrib['name'])
                        Country.append(x.attrib['name'])



import pandas as pd
import requests

url="<URL TO DOWNLOAD.CSV>"
s=requests.get(url).content
c=pd.read_csv(s)